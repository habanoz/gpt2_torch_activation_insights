{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_saved_block_activations(B,S,D,NH, amp=False, dropout=False, bias=False):\n",
    "    bo = 1\n",
    "    fp = 4\n",
    "    mp = 2 if amp else 4\n",
    "    hp = 2\n",
    "    i64 = 8\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    ## layer norm1\n",
    "\n",
    "    # NativeLayerNormBackward0-input\n",
    "    total += B*S*D * fp # input tensor, typically not cached previously. \n",
    "    # NativeLayerNormBackward0-result1\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-result2\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-weight\n",
    "    total += 0 # model parameter at full precision, no need to account for. \n",
    "    # NativeLayerNormBackward0-bias\n",
    "    total += int(bias) * 0 * fp # amp does not affect ln and full precision weights are not added to total, because it is part of parameter weights. \n",
    "    \n",
    "\n",
    "    print(\"LN1\", total)\n",
    "    last = total\n",
    "\n",
    "    ## layer norm2\n",
    "\n",
    "    # NativeLayerNormBackward0-input\n",
    "    total += B*S*D * fp # this tensor comes from addition of attention and input tensor\n",
    "    # NativeLayerNormBackward0-result1\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-result2\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-weight\n",
    "    total += 0 # model parameter at full precision, no need to account for. \n",
    "    # NativeLayerNormBackward0-bias\n",
    "    total +=int(bias) *  0 * fp # amp does not affect ln and full precision weights are not added to total, because it is part of parameter weights.\n",
    "    \n",
    "    print(\"LN2\", total-last)\n",
    "    last = total\n",
    "\n",
    "    ## scaled_dot_product_attention efficient implmenentation\n",
    "\n",
    "    # MmBackward0-self (c_attn)\n",
    "    total += B*S*D * mp\n",
    "    # MmBackward0-mat2 (c_attn)\n",
    "    total += 3*D*D * hp if amp else 0 # model parameter (attn.c_attn.weight) at half precision, or full precision\n",
    "\n",
    "        \n",
    "    print(\"attention-c_attn\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # ScaledDotProductEfficientAttentionBackward0-value-query-key\n",
    "    total += B*S*D * mp\n",
    "    total += B*S*D * mp\n",
    "    total += B*S*D * mp\n",
    "    # ScaledDotProductEfficientAttentionBackward0-log_sumexp\n",
    "    total += B*NH*S * fp\n",
    "    # ScaledDotProductEfficientAttentionBackward0-philox_offset\n",
    "    total += 1 * i64\n",
    "    # ScaledDotProductEfficientAttentionBackward0-philox_seed\n",
    "    total += 1 * i64\n",
    "    # ScaledDotProductEfficientAttentionBackward0-output\n",
    "    ## total += B*S*D * mp # this is accounted in c_proj matmul\n",
    "\n",
    "    print(\"scaled dot product attention\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # MmBackward0-mat2 (c_proj)\n",
    "    total += D * D * hp if amp else 0  # model parameter (attn.c_proj.weight) at half precision, or full precision.\n",
    "    # MmBackward0-self (c_proj)\n",
    "    total += B*S*D * mp # same as ScaledDotProductEfficientAttentionBackward0-output\n",
    "\n",
    "    print(\"attention-c_proj\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # NativeDropoutBackward0-result1\n",
    "    total += B*S*D * bo if dropout else 0 # dropout mask for resid_dropout\n",
    "\n",
    "    print(\"attention-droput\", total-last)\n",
    "    last = total\n",
    "\n",
    "    ## MLP\n",
    "\n",
    "    # MmBackward0-mat2 (c_fc)\n",
    "    total += S*D*hp if amp else 0 # model parameter (mlp.c_fc.weight) at half precision, or full precision\n",
    "    # MmBackward0-self (c_fc)\n",
    "    total += B*S*D * mp # this value comes from layer norm 2\n",
    "\n",
    "    print(\"c_fc\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # GeluBackward0-self\n",
    "    total += 4 * B * S * D * mp # this tensor is result of 'x @ c_fc'\n",
    "\n",
    "    print(\"gelu\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # MmBackward0-mat2 (c_proj)\n",
    "    total += S*D*hp if amp else 0 # model parameter (mlp.c_proj.weight) at half precision, or full precision\n",
    "    # MmBackward0-self (c_proj)\n",
    "    total += 4*B * S * D * mp  # this value comes from GELU\n",
    "\n",
    "    print(\"c_proj\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # NativeDropoutBackward0-result1 (mlp dropout)\n",
    "    total += B*S*D * bo if dropout else 0 # dropout mask for resid_dropout\n",
    "\n",
    "    print(\"mlp dropout\", total-last)\n",
    "    last = total\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_saved_gpt_activations(B,S,D,NH,V, n_layers=1, amp=False, dropout=False, bias=False):\n",
    "    bo = 1\n",
    "    fp = 4\n",
    "    mp = 2 if amp else 4\n",
    "    hp = 2\n",
    "    i64 = 8\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    ## token embeddings\n",
    "    total += B * S * i64 # indices cached by embedding layer\n",
    "\n",
    "    ## position embeddings\n",
    "    total += S * i64 # indices cached by embedding layer\n",
    "\n",
    "    print(\"embeddings\", total)\n",
    "    last = total\n",
    "\n",
    "    ## x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    total += B*S*D * bo if dropout else 0 # dropout mask for resid_dropout\n",
    "    \n",
    "    print(\"gpt-droput\", total-last)\n",
    "    last = total\n",
    "    \n",
    "    # add blocks\n",
    "    \n",
    "    ## we shifted calculation to add input to first layer norm\n",
    "    ## total += B *S *D * fp # cache input, normally it is cached as the input variable to the first layer norm. \n",
    "    total_block= n_layers * calc_block(B,S,D,NH,amp,dropout, bias)\n",
    "    total +=total_block\n",
    "    print(\"block_sum\",str(total_block))\n",
    "\n",
    "    print(\"total blocks (block+input)\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # final ln\n",
    "    # NativeLayerNormBackward0-input\n",
    "    total += B*S*D * fp # output of final attention block\n",
    "    # NativeLayerNormBackward0-result1\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-result2\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-weight\n",
    "    total += 0 # model parameter at full precision, no need to account for. \n",
    "    # NativeLayerNormBackward0-bias\n",
    "    total +=int(bias) *  0 * fp # amp does not affect ln and full precision weights are not added to total, because it is part of parameter weights.\n",
    "    \n",
    "    print(\"ln_f\", total-last)\n",
    "    last = total\n",
    "\n",
    "    ## lm head\n",
    "    # lm_head-mat1\n",
    "    total += V * D * hp if amp else 0 # in amp half preicison copy of embedding parameter weights are used. in full precision embedding parameter weights is used, no need to add to total\n",
    "    # lm_head-mat1\n",
    "    total += B*S*D * mp # output of final ln\n",
    "    \n",
    "    print(\"lm_head\", total-last)\n",
    "    last = total\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(sequenceLength, hiddenDim, vocabSize, numLayers=1, includeBias=False, includeEmbeddings=False, weightTying=True):\n",
    "    wte = vocabSize * hiddenDim # Token embedding\n",
    "    wpe = hiddenDim * sequenceLength # Positional embedding\n",
    "    \n",
    "    # Per layer\n",
    "    ln_1 = hiddenDim # layer norm 1\n",
    "    attn_c_attn = 3 * hiddenDim * hiddenDim # Query, Key, Value projections\n",
    "    attn_c_proj = hiddenDim * hiddenDim # Output projection\n",
    "    ln_2 = hiddenDim # layer norm 2\n",
    "    mlp_c_fc = 4 * hiddenDim * hiddenDim # First MLP layer\n",
    "    mlp_c_proj = 4 * hiddenDim * hiddenDim # Second MLP layer\n",
    "\n",
    "    paramsPerLayer = ln_1 + attn_c_attn + attn_c_proj + ln_1 + mlp_c_fc + mlp_c_proj\n",
    "    final_ln = hiddenDim\n",
    "    final_dense = vocabSize * hiddenDim\n",
    "\n",
    "    totalParamsWOEmbedding = (paramsPerLayer * numLayers) + final_ln + final_dense\n",
    "    totalParams = totalParamsWOEmbedding\n",
    "    \n",
    "    if includeBias:\n",
    "        # each layer has 6 biases\n",
    "        # - ln_1.bias\n",
    "        # - c_attn.bias (3 * hidden dimension)\n",
    "        # - c_proj.bias\n",
    "        # - ln_2.bias\n",
    "        # - c_fc.bias (4 * hidden dimension)\n",
    "        # - c_proj.bias\n",
    "        totalParams += 11 * numLayers * hiddenDim\n",
    "        \n",
    "        # final ln has a bias\n",
    "        totalParams += hiddenDim\n",
    "\n",
    "    if includeEmbeddings:\n",
    "        totalParams += wpe\n",
    "        if not weightTying:\n",
    "            # if weight tying, wte and final_dense is shared. Only count wte when weight tying is off\n",
    "            totalParams += wte\n",
    "    \n",
    "    return totalParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4920960"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_params(512, 384, 8192, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_all(B,S,D,NH,V, n_layers=1, amp=False, dropout=False, bias=False):\n",
    "    bo = 1\n",
    "    fp = 4\n",
    "    mp = 2 if amp else 4\n",
    "    hp = 2\n",
    "    i64 = 8\n",
    "\n",
    "    total = 0\n",
    "    saved_activations = calc_gpt(B,S,D,NH,V, n_layers, amp, dropout, bias)\n",
    "    print(\"gpt activations\", saved_activations)\n",
    "\n",
    "    total += saved_activations\n",
    "    last = total\n",
    "    \n",
    "    # x and y\n",
    "    total += B*S*i64 * 2\n",
    "\n",
    "    print(\"inputs\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # output of lm head, before loss is calculated.\n",
    "    # this tensor will be destroyed after loss calculation, however, it is still a significant contribution.\n",
    "    total += B*S*V*mp\n",
    "\n",
    "    print(\"lm_head_output\", total-last)\n",
    "    last = total\n",
    "\n",
    "    params = calc_params(S,D,V,n_layers, includeBias=bias, includeEmbeddings=True, weightTying=True)\n",
    "    print(\"Number of Params\", params)\n",
    "\n",
    "    total += params*fp\n",
    "\n",
    "    print(\"params_mem\", total-last)\n",
    "    last = total\n",
    "\n",
    "    total += params*fp\n",
    "\n",
    "    print(\"gradients\", total-last)\n",
    "    last = total\n",
    "\n",
    "    total += params*fp*2\n",
    "    \n",
    "    print(\"optimizer\", total-last)\n",
    "    last = total\n",
    "\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LN1 37945344\n",
      "LN2 37945344\n",
      "attention-c_attn 37748736\n",
      "scaled dot product attention 113639440\n",
      "attention-c_proj 37748736\n",
      "attention-droput 9437184\n",
      "c_fc 37748736\n",
      "gelu 150994944\n",
      "c_proj 150994944\n",
      "mlp dropout 9437184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "623640592"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_saved_block_activations(B=48,S=512,D=384,NH=4, amp=False, dropout=True, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings 401408\n",
      "gpt-droput 25165824\n",
      "LN1 101056512\n",
      "LN2 101056512\n",
      "attention-c_attn 100663296\n",
      "scaled dot product attention 302776336\n",
      "attention-c_proj 100663296\n",
      "attention-droput 25165824\n",
      "c_fc 100663296\n",
      "gelu 402653184\n",
      "c_proj 402653184\n",
      "mlp dropout 25165824\n",
      "block_sum 13300138112\n",
      "total blocks (block+input) 13300138112\n",
      "ln_f 101056512\n",
      "lm_head 100663296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13527425152"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_saved_gpt_activations(B=48,S=1024,D=512,NH=4,V=8192, n_layers=8, amp=False, dropout=True, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings 204800\n",
      "gpt-droput 9437184\n",
      "LN1 37945344\n",
      "LN2 37945344\n",
      "attention-c_attn 19759104\n",
      "scaled dot product attention 57016336\n",
      "attention-c_proj 19169280\n",
      "attention-droput 9437184\n",
      "c_fc 19660800\n",
      "gelu 75497472\n",
      "c_proj 76283904\n",
      "mlp dropout 9437184\n",
      "block_sum 1448607808\n",
      "total blocks (block+input) 1448607808\n",
      "ln_f 37945344\n",
      "lm_head 25165824\n",
      "gpt activations 1521360960\n",
      "inputs 393216\n",
      "lm_head_output 402653184\n",
      "Number of Params 10620288\n",
      "params_mem 42481152\n",
      "gradients 42481152\n",
      "optimizer 84962304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2094331968"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_all(B=24,S=1024,D=384,NH=4,V=8192, n_layers=4, amp=True, dropout=True, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
