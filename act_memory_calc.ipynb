{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_block(B,S,D,NH, amp=False, dropout=False, bias=False):\n",
    "    bo = 1\n",
    "    fp = 4\n",
    "    mp = 2 if amp else 4\n",
    "    hp = 2\n",
    "    i64 = 8\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    ## layer norm1\n",
    "\n",
    "    # NativeLayerNormBackward0-input\n",
    "    total += 0 * fp # input tensor, assume already cached previously. \n",
    "    # NativeLayerNormBackward0-result1\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-result2\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-weight\n",
    "    total += 0 # model parameter at full precision, no need to account for. \n",
    "    # NativeLayerNormBackward0-bias\n",
    "    total += int(bias) * 0 * fp # amp does not affect ln and full precision weights are not added to total, because it is part of parameter weights. \n",
    "    \n",
    "\n",
    "    print(\"LN1\", total)\n",
    "    last = total\n",
    "\n",
    "    ## layer norm2\n",
    "\n",
    "    # NativeLayerNormBackward0-input\n",
    "    total += B*S*D * fp # this tensor comes from addition of attention and input tensor\n",
    "    # NativeLayerNormBackward0-result1\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-result2\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-weight\n",
    "    total += 0 # model parameter at full precision, no need to account for. \n",
    "    # NativeLayerNormBackward0-bias\n",
    "    total +=int(bias) *  0 * fp # amp does not affect ln and full precision weights are not added to total, because it is part of parameter weights.\n",
    "    \n",
    "    print(\"LN2\", total-last)\n",
    "    last = total\n",
    "\n",
    "    ## scaled_dot_product_attention efficient implmenentation\n",
    "\n",
    "    # MmBackward0-self (c_attn)\n",
    "    total += B*S*D * mp\n",
    "    # MmBackward0-mat2 (c_attn)\n",
    "    total += 3*D*D * hp if amp else 0 # model parameter (attn.c_attn.weight) at half precision, or full precision\n",
    "\n",
    "        \n",
    "    print(\"attention-c_attn\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # ScaledDotProductEfficientAttentionBackward0-value-query-key\n",
    "    total += B*S*D * mp\n",
    "    total += B*S*D * mp\n",
    "    total += B*S*D * mp\n",
    "    # ScaledDotProductEfficientAttentionBackward0-log_sumexp\n",
    "    total += B*NH*S * fp\n",
    "    # ScaledDotProductEfficientAttentionBackward0-philox_offset\n",
    "    total += 1 * i64\n",
    "    # ScaledDotProductEfficientAttentionBackward0-philox_seed\n",
    "    total += 1 * i64\n",
    "    # ScaledDotProductEfficientAttentionBackward0-output\n",
    "    total += B*S*D * mp\n",
    "\n",
    "    print(\"scaled dot product attention\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # MmBackward0-mat2 (c_proj)\n",
    "    total += D * D * hp if amp else 0  # model parameter (attn.c_proj.weight) at half precision, or full precision.\n",
    "    # MmBackward0-self (c_proj)\n",
    "    total += 0 # ScaledDotProductEfficientAttentionBackward0-output, already cached by the scaled_dot_product_attention block.\n",
    "\n",
    "    print(\"attention-c_proj\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # NativeDropoutBackward0-result1\n",
    "    total += B*S*D * bo if dropout else 0 # dropout mask for resid_dropout\n",
    "\n",
    "    print(\"attention-droput\", total-last)\n",
    "    last = total\n",
    "\n",
    "    ## MLP\n",
    "\n",
    "    # MmBackward0-mat2 (c_fc)\n",
    "    total += S*D*hp if amp else 0 # model parameter (mlp.c_fc.weight) at half precision, or full precision\n",
    "    # MmBackward0-self (c_fc)\n",
    "    total += B*S*D * mp # this value comes from layer norm 2\n",
    "\n",
    "    print(\"c_fc\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # GeluBackward0-self\n",
    "    total += B * S * S * mp # this tensor is result of 'x @ c_fc'\n",
    "\n",
    "    print(\"gelu\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # MmBackward0-mat2 (c_proj)\n",
    "    total += S*D*hp if amp else 0 # model parameter (mlp.c_proj.weight) at half precision, or full precision\n",
    "    # MmBackward0-self (c_proj)\n",
    "    total += B * S * S * mp  # this value comes from GELU\n",
    "\n",
    "    print(\"c_proj\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # NativeDropoutBackward0-result1 (mlp dropout)\n",
    "    total += B*S*D * bo if dropout else 0 # dropout mask for resid_dropout\n",
    "\n",
    "    print(\"mlp dropout\", total-last)\n",
    "    last = total\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gpt(B,S,D,NH,V, n_layers=1, amp=False, dropout=False, bias=False):\n",
    "    bo = 1\n",
    "    fp = 4\n",
    "    mp = 2 if amp else 4\n",
    "    hp = 2\n",
    "    i64 = 8\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    ## token embeddings\n",
    "    total += B * S * i64 # indices cached by embedding layer\n",
    "\n",
    "    ## position embeddings\n",
    "    total += S * i64 # indices cached by embedding layer\n",
    "\n",
    "    print(\"embeddings\", total)\n",
    "    last = total\n",
    "\n",
    "    ## x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    total += B*S*D * bo if dropout else 0 # dropout mask for resid_dropout\n",
    "    \n",
    "    print(\"gpt-droput\", total-last)\n",
    "    last = total\n",
    "    \n",
    "    # add blocks\n",
    "    total += B *S *D * fp # cache input, normally it is cached as the input variable to the first layer norm. \n",
    "    total_block= n_layers * calc_block(B,S,D,NH,amp,dropout, bias)\n",
    "    total +=total_block\n",
    "    print(\"total block\",str(total_block))\n",
    "\n",
    "    print(\"block\", total-last)\n",
    "    last = total\n",
    "\n",
    "    # final ln\n",
    "    # NativeLayerNormBackward0-input\n",
    "    total += B*S*D * fp # output of final attention block\n",
    "    # NativeLayerNormBackward0-result1\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-result2\n",
    "    total += B*S * fp\n",
    "    # NativeLayerNormBackward0-weight\n",
    "    total += 0 # model parameter at full precision, no need to account for. \n",
    "    # NativeLayerNormBackward0-bias\n",
    "    total +=int(bias) *  0 * fp # amp does not affect ln and full precision weights are not added to total, because it is part of parameter weights.\n",
    "    \n",
    "    print(\"ln_f\", total-last)\n",
    "    last = total\n",
    "\n",
    "    ## lm head\n",
    "    # lm_head-mat1\n",
    "    total += V * D * hp if amp else 0 # in amp half preicison copy of embedding parameter weights are used. in full precision embedding parameter weights is used, no need to add to total\n",
    "    # lm_head-mat1\n",
    "    total += B*S*D * mp # output of final ln\n",
    "    \n",
    "    print(\"lm_head\", total-last)\n",
    "    last = total\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LN1 65536\n",
      "LN2 33619968\n",
      "attention-c_attn 33554432\n",
      "scaled dot product attention 134283280\n",
      "attention-c_proj 0\n",
      "attention-droput 0\n",
      "c_fc 33554432\n",
      "gelu 134217728\n",
      "c_proj 134217728\n",
      "mlp dropout 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "503513104"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_block(B=2,S=4096,D=1024,NH=2, amp=False, dropout=False, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings 98304\n",
      "gpt-droput 8388608\n",
      "LN1 65536\n",
      "LN2 33619968\n",
      "attention-c_attn 23068672\n",
      "scaled dot product attention 67174416\n",
      "attention-c_proj 2097152\n",
      "attention-droput 8388608\n",
      "c_fc 25165824\n",
      "gelu 67108864\n",
      "c_proj 75497472\n",
      "mlp dropout 8388608\n",
      "total block 310575120\n",
      "block 344129552\n",
      "ln_f 33619968\n",
      "lm_head 16809984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403046416"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_gpt(B=2,S=4096,D=1024,NH=2,V=16, n_layers=1, amp=True, dropout=True, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
